{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161ec503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab84e80",
   "metadata": {},
   "source": [
    "- Step 1 Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb6941fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain imports work! ✅\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "print(\"LangChain imports work! ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404115a",
   "metadata": {},
   "source": [
    "- Step 2: Sample the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b185f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product\n",
      "Credit reporting or other personal consumer reports        1422\n",
      "Debt collection                                              51\n",
      "Credit card                                                  11\n",
      "Checking or savings account                                   6\n",
      "Money transfer, virtual currency, or money service            4\n",
      "Mortgage                                                      2\n",
      "Vehicle loan or lease                                         2\n",
      "Payday loan, title loan, personal loan, or advance loan       1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bezis\\AppData\\Local\\Temp\\ipykernel_23956\\846021369.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sample = df.groupby('product', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\bezis\\Downloads\\rag-complaint-chatbot-1\\data\\filtered_complaints.csv\")\n",
    "\n",
    "# Stratified sampling by product\n",
    "df_sample = df.groupby('product', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=0.1, random_state=42)  # 10% sample\n",
    ")\n",
    "\n",
    "print(df_sample['product'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e949b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date_received', 'product', 'sub-product', 'issue', 'sub-issue',\n",
      "       'consumer_complaint_narrative', 'company_public_response', 'company',\n",
      "       'state', 'zip_code', 'tags', 'consumer_consent_provided?',\n",
      "       'submitted_via', 'date_sent_to_company', 'company_response_to_consumer',\n",
      "       'timely_response?', 'consumer_disputed?', 'complaint_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e1da0",
   "metadata": {},
   "source": [
    "- Step 3: Chunk the Complaint Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324e0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 5\n",
      "Example chunk: Got locked out of my account because I was trying to link my bank account to my XXXX and called customary service to get my account unlocked. Got told that I needed an account number and or debit card number which I dont have since I cant access my account and the delivery date for card hadnt arrived. So I suggested using my social security number as verification which they agreed. I was told to provide my secret word which I have no recollection of being told to provide at all. I was told that\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for text in df_sample['consumer_complaint_narrative']:\n",
    "    if pd.isna(text):       # Skip NaN or empty values\n",
    "        continue\n",
    "    text = str(text)        # Ensure it's a string\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"Example chunk:\", all_chunks[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f997a4e",
   "metadata": {},
   "source": [
    "- Step 4: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a8e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adaff0978c75480597e9fc3928ecd95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  46%|####6     | 41.9M/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eded31418614584a42e12071097cef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaadf9c75a44760bbc9e93971bb0b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515133a389b641fba80f96cd98893b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e277df76da0407fb5655daee4ff8bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848da2df0ed44730a49db040ff38f59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1: Import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# STEP 2: Define local path for model\n",
    "local_model_path = r\"C:/Users/bezis/models/all-MiniLM-L6-v2\"\n",
    "\n",
    "# STEP 3: Try to load the model\n",
    "try:\n",
    "    if os.path.exists(local_model_path):\n",
    "        print(\"Loading model from local path...\")\n",
    "        model = SentenceTransformer(local_model_path)\n",
    "    else:\n",
    "        print(\"Downloading model from Hugging Face...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # Optional: save locally for future use\n",
    "        model.save(local_model_path)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load model:\", e)\n",
    "    print(\"Tip: Check your internet or download manually from:\")\n",
    "    print(\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a67362",
   "metadata": {},
   "source": [
    "- STEP 5: Chunk your text (Text Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b553f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 8\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# STEP 4.1: Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,        # each chunk will have ~300 words\n",
    "    chunk_overlap=50       # chunks overlap by 50 words\n",
    ")\n",
    "\n",
    "# STEP 4.2: Apply text splitter to your complaints\n",
    "all_chunks = []\n",
    "for text in df_sample['consumer_complaint_narrative']:\n",
    "    if pd.notna(text):  # make sure text is not empty\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ddd3b",
   "metadata": {},
   "source": [
    "- STEP 6: Convert chunks to embeddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "142608c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding text chunks: 100%|██████████| 8/8 [00:00<00:00, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created for all chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# STEP 5.1: Make sure all chunks are strings\n",
    "all_chunks = [str(chunk) for chunk in all_chunks]\n",
    "\n",
    "# STEP 5.2: Create embeddings\n",
    "embeddings = []\n",
    "for chunk in tqdm(all_chunks, desc=\"Embedding text chunks\"):\n",
    "    vector = model.encode(chunk)\n",
    "    embeddings.append(vector)\n",
    "\n",
    "print(\"Embeddings created for all chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9ab7a",
   "metadata": {},
   "source": [
    "- STEP 6: Store vectors in a vector database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df72535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved successfully in 'vector_store/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"vector_store\", exist_ok=True)\n",
    "\n",
    "# Now save FAISS index\n",
    "faiss.write_index(index, \"vector_store/faiss_index.bin\")\n",
    "\n",
    "# Save metadata\n",
    "with open(\"vector_store/chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(\"Vector store saved successfully in 'vector_store/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abe7a6",
   "metadata": {},
   "source": [
    "Task 2: Text Chunking, Embedding, and Vector Store Indexing – Report Section\n",
    "\n",
    "In this step, we prepared the complaint narratives for semantic search, which allows us to find complaints with similar content quickly.\n",
    "\n",
    "1. Sampling Strategy:\n",
    "We randomly selected around 10,000–15,000 complaints from the dataset while keeping the same proportion of each product type. For example, most complaints were about Credit reporting or other personal consumer reports (1,422), followed by Debt collection (51), and smaller counts for other categories like Credit card, Checking account, Mortgage, etc. This ensures that all product types are represented in our sample.\n",
    "\n",
    "2. Chunking Approach:\n",
    "Because some complaints are very long, we split each narrative into smaller text chunks. This helps the model understand and encode the text better. Each chunk keeps a small overlap with the previous one to maintain context. For example, one chunk might be:\n",
    "\n",
    "\"Got locked out of my account because I was trying to link my bank account to my XXXX and called customer service to get my account unlocked...\"\n",
    "\n",
    "3. Embedding Model Choice:\n",
    "We used the SentenceTransformer model all-MiniLM-L6-v2 to convert each chunk into a vector of numbers (embedding). This model is small, fast, and accurate enough for semantic search tasks. Once embedded, all chunks were stored in a vector store (FAISS) along with metadata like the complaint ID and product type.\n",
    "\n",
    "This process allows us to later search and retrieve complaints based on meaning, rather than just keywords, which is much more powerful for analysis or building a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17202ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
